{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8469fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47dec4",
   "metadata": {},
   "source": [
    "# NOVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8255342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(15)\n",
    "\n",
    "url = 'https://www.news.com.au/'\n",
    "driver.get(url)\n",
    "\n",
    "xpath_searchButton = '/html/body/nav/form/button'\n",
    "xpath_searchBox = '/html/body/nav/form/input'\n",
    "\n",
    "xpath_filter_tab = '//*[@id=\"refine\"]'\n",
    "\n",
    "xpath_type_all = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[1]/label'\n",
    "xpath_type_audio = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[3]/label'\n",
    "xpath_type_video = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[5]/label'\n",
    "xpath_type_article = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[2]/label'\n",
    "xpath_type_gallery = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[4]/label'\n",
    "\n",
    "xpath_time_anytime = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[1]/label'\n",
    "xpath_time_past_day = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[2]/label'\n",
    "xpath_time_past_week = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[3]/label'\n",
    "xpath_time_past_month = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[4]/label'\n",
    "xpath_time_past_year = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[5]/label'\n",
    "\n",
    "query = 'healthcare'\n",
    "\n",
    "search_button = driver.find_element(By.XPATH, xpath_searchButton)\n",
    "search_button.click()\n",
    "print(\"Botão de busca clicado!\")\n",
    "\n",
    "search_box = driver.find_element(By.XPATH, xpath_searchBox)\n",
    "search_box.send_keys(query)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "print(\"Pesquisa enviada!\")\n",
    "time.sleep(5)\n",
    "\n",
    "if driver.find_element(By.XPATH, xpath_filter_tab):\n",
    "    filter_tab = driver.find_element(By.XPATH, xpath_filter_tab)\n",
    "    filter_tab.click()\n",
    "\n",
    "filter_tab = driver.find_element(By.XPATH, xpath_type_article)\n",
    "filter_tab.click()\n",
    "time.sleep(3)\n",
    "filter_tab = driver.find_element(By.XPATH, xpath_time_past_month)\n",
    "filter_tab.click()\n",
    "print('Começando as pesquisas........\\n--------------')\n",
    "time.sleep(3)\n",
    "\n",
    "if driver.find_element(By.XPATH, xpath_filter_tab):\n",
    "    filter_tab = driver.find_element(By.XPATH, xpath_filter_tab)\n",
    "    filter_tab.click()\n",
    "\n",
    "time.sleep(2)\n",
    "# Localiza todos os artigos na página\n",
    "articles = driver.find_elements(By.CLASS_NAME, 'storyblock')\n",
    "\n",
    "# Define as frases de busca\n",
    "search_phrases = [query]  # Exemplo de frases de busca\n",
    "\n",
    "# Cria uma pasta para armazenar as imagens\n",
    "os.makedirs('news_images', exist_ok=True)\n",
    "\n",
    "# Lista para armazenar os dados de todos os artigos\n",
    "articles_data = []\n",
    "\n",
    "# Itera sobre cada artigo e extrai os dados\n",
    "for article in articles:\n",
    "    # Extrai o título\n",
    "    title_element = article.find_element(By.CLASS_NAME, 'storyblock_title')\n",
    "    title = title_element.text\n",
    "\n",
    "    # Extrai a data\n",
    "    date_element = article.find_element(By.CLASS_NAME, 'storyblock_datetime')\n",
    "    date = date_element.get_attribute('datetime')\n",
    "\n",
    "    # Extrai a descrição, se disponível\n",
    "    description_element = article.find_elements(By.CLASS_NAME, 'storyblock_standfirst')\n",
    "    description = description_element[0].text if description_element else None\n",
    "\n",
    "    # Extrai o nome do arquivo da imagem\n",
    "    image_element = article.find_element(By.CLASS_NAME, 'responsive-img_img')\n",
    "    image_url = image_element.get_attribute('src')\n",
    "\n",
    "    # Faz uma requisição para obter os dados da imagem\n",
    "    response = requests.get(image_url)\n",
    "    \n",
    "    # Determina a extensão do arquivo\n",
    "    content_type = response.headers.get('Content-Type')\n",
    "    if 'image/jpeg' in content_type:\n",
    "        extension = '.jpg'\n",
    "    elif 'image/png' in content_type:\n",
    "        extension = '.png'\n",
    "    else:\n",
    "        extension = ''  # Caso algum outro formato seja encontrado\n",
    "    \n",
    "    # Extrai o nome do arquivo da URL\n",
    "    parsed_url = urlparse(image_url)\n",
    "    image_filename = os.path.join('news_images', os.path.basename(parsed_url.path) + extension)\n",
    "\n",
    "    # Salva a imagem\n",
    "    with open(image_filename, 'wb') as handler:\n",
    "        handler.write(response.content)\n",
    "\n",
    "    # Conta as frases de busca no título e na descrição\n",
    "    phrase_count = sum(title.lower().count(phrase.lower()) for phrase in search_phrases) + \\\n",
    "                   (sum(description.lower().count(phrase.lower()) for phrase in search_phrases) if description else 0)\n",
    "\n",
    "    # Verifica se o título ou descrição contém algum valor monetário\n",
    "    money_pattern = re.compile(r'(\\$\\d+(\\.\\d{1,2})?)|(\\d+\\s?(dollars|USD))')\n",
    "    contains_money = bool(money_pattern.search(title) or (description and money_pattern.search(description)))\n",
    "\n",
    "    # Armazena os dados do artigo\n",
    "    articles_data.append({\n",
    "        \"Title\": title,\n",
    "        \"Date\": date,\n",
    "        \"Description\": description,\n",
    "        \"Picture Filename\": image_filename,\n",
    "        \"Count of Search Phrases\": phrase_count,\n",
    "        \"Contains Money\": contains_money\n",
    "    })\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "# Exibe os resultados de todos os artigos\n",
    "for article_data in articles_data:\n",
    "    print(article_data)\n",
    "print('Pesquisa finalizada')\n",
    "\n",
    "# Cria um DataFrame com os dados coletados\n",
    "df = pd.DataFrame(articles_data)\n",
    "\n",
    "# Salva o DataFrame em um arquivo Excel\n",
    "df.to_excel('news_data.xlsx', index=False)\n",
    "\n",
    "# Fecha o navegador\n",
    "driver.quit()\n",
    "\n",
    "print(\"Processo concluído. Dados salvos em 'news_data.xlsx' e imagens baixadas em 'news_images/'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523d664",
   "metadata": {},
   "source": [
    "# *CLASSES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b289b057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 17:44:48,122 - INFO - Botão de busca clicado!\n",
      "2024-08-14 17:44:58,451 - INFO - Pesquisa enviada!\n",
      "2024-08-14 17:45:03,680 - INFO - Filtro de categoria 'article' aplicado!\n",
      "2024-08-14 17:45:03,788 - INFO - Filtro de tempo aplicado para os últimos 3 meses!\n",
      "2024-08-14 17:45:36,189 - INFO - Dados salvos em 'news_data.xlsx'.\n",
      "2024-08-14 17:45:38,621 - INFO - Processo concluído.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import configparser\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Configuração de logs\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self, config_file='config.ini'):\n",
    "        self.config = configparser.ConfigParser()\n",
    "        self.config.read(config_file)\n",
    "        self.query = self.config.get('search', 'query')\n",
    "        self.category = self.config.get('filters', 'category')\n",
    "        self.time = int(self.config.get('filters', 'time'))\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.implicitly_wait(15)\n",
    "\n",
    "    def search_news(self):\n",
    "        url = 'https://www.news.com.au/'\n",
    "        self.driver.get(url)\n",
    "\n",
    "        try:\n",
    "            search_button = self.driver.find_element(By.XPATH, '/html/body/nav/form/button')\n",
    "            search_button.click()\n",
    "            logging.info(\"Search button clicked.\")\n",
    "\n",
    "            search_box = self.driver.find_element(By.XPATH, '/html/body/nav/form/input')\n",
    "            search_box.send_keys(self.query)\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            logging.info(\"Search submitted.\")\n",
    "            sleep(5)\n",
    "\n",
    "            filter_tab = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"refine\"]'))\n",
    "            )\n",
    "            filter_tab.click()\n",
    "\n",
    "            if self.category == 'article':\n",
    "                category_xpath = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[2]/label'\n",
    "            elif self.category == 'audio':\n",
    "                category_xpath = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[3]/label'\n",
    "            elif self.category == 'gallery':\n",
    "                category_xpath = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[4]/label'\n",
    "            elif self.category == 'video':\n",
    "                category_xpath = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[5]/label'\n",
    "            else:\n",
    "                category_xpath = '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[1]/label'  # Default to 'all'\n",
    "\n",
    "            category_filter = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, category_xpath))\n",
    "            )\n",
    "            category_filter.click()\n",
    "            logging.info(f\"Category filter '{self.category}' applied.\")\n",
    "\n",
    "            if self.time == 'day':\n",
    "                time_xpath = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[2]/label'\n",
    "\n",
    "            elif self.time == 'week':\n",
    "                time_xpath = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[3]/label'\n",
    "\n",
    "            elif self.time == 'month':\n",
    "                time_xpath = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[4]/label'\n",
    "\n",
    "            elif self.time == 'year':\n",
    "                time_xpath = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[5]/label'\n",
    "\n",
    "            else:\n",
    "                time_xpath = '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[1]/label'\n",
    "\n",
    "            time_filter = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, time_xpath))\n",
    "            )\n",
    "            time_filter.click()\n",
    "            logging.info(f\"Time filter applied for the last {self.time}.\")\n",
    "            sleep(3)\n",
    "\n",
    "            filter_tab.click()\n",
    "            sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error applying the filters: {e}\")\n",
    "            self.driver.quit()\n",
    "            raise\n",
    "\n",
    "    def download_and_save_image(image_url, filename, format='JPEG'):\n",
    "        try:\n",
    "            # Baixa a imagem da URL\n",
    "            with urllib.request.urlopen(image_url) as response:\n",
    "                image_data = response.read()\n",
    "            \n",
    "            # Abre a imagem em memória\n",
    "            image = Image.open(BytesIO(image_data))\n",
    "            \n",
    "            # Salva a imagem com o nome e formato fornecidos\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)  # Certifica-se de que o diretório existe\n",
    "            image.save(filename, format=format)  # Salva a imagem com o formato especificado\n",
    "            \n",
    "            print(f\"Image save as '{filename}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar ou salvar a imagem: {e}\")\n",
    "\n",
    "    def process_articles(self):\n",
    "        articles = self.driver.find_elements(By.CLASS_NAME, 'storyblock')\n",
    "        os.makedirs('news_images', exist_ok=True)\n",
    "        articles_data = []\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                title_element = article.find_element(By.CLASS_NAME, 'storyblock_title')\n",
    "                title = title_element.text\n",
    "\n",
    "                date_element = article.find_element(By.CLASS_NAME, 'storyblock_datetime')\n",
    "                date = date_element.get_attribute('datetime')\n",
    "\n",
    "                description_element = article.find_elements(By.CLASS_NAME, 'storyblock_standfirst')\n",
    "                description = description_element[0].text if description_element else None\n",
    "\n",
    "                image_element = article.find_element(By.CLASS_NAME, 'responsive-img_img')\n",
    "                image_url = image_element.get_attribute('src')\n",
    "\n",
    "                # response = requests.get(image_url)\n",
    "                # content_type = response.headers.get('Content-Type')\n",
    "\n",
    "                # extension = '.jpg' if 'jpeg' in content_type else '.png' if 'png' in content_type else ''\n",
    "                parsed_url = urlparse(image_url)\n",
    "                image_filename = os.path.join('news_images', os.path.basename(parsed_url.path) + \".jpeg\")\n",
    "\n",
    "                # with open(image_filename, 'wb') as handler:\n",
    "                #     handler.write(response.content)\n",
    "\n",
    "                download_and_save_image(image_url, image_filename)\n",
    "\n",
    "                search_phrases = [self.query]\n",
    "                phrase_count = sum(title.lower().count(phrase.lower()) for phrase in search_phrases) + \\\n",
    "                               (sum(description.lower().count(phrase.lower()) for phrase in search_phrases) if description else 0)\n",
    "\n",
    "                money_pattern = re.compile(r'(\\$\\d+(\\.\\d{1,2})?)|(\\d+\\s?(dollars|USD))')\n",
    "                contains_money = bool(money_pattern.search(title) or (description and money_pattern.search(description)))\n",
    "\n",
    "                articles_data.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Date\": date,\n",
    "                    \"Description\": description,\n",
    "                    \"Picture Filename\": image_filename,\n",
    "                    \"Count of Search Phrases\": phrase_count,\n",
    "                    \"Contains Money\": contains_money\n",
    "                })\n",
    "\n",
    "                sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing the article: {e}\")\n",
    "\n",
    "        return articles_data\n",
    "\n",
    "    def save_to_excel(self, articles_data):\n",
    "        df = pd.DataFrame(articles_data)\n",
    "        df.to_excel('news_data.xlsx', index=False)\n",
    "        logging.info(\"Data saved in 'news_data.xlsx'.\")\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.search_news()\n",
    "            articles_data = self.process_articles()\n",
    "            self.save_to_excel(articles_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in the scraping process: {e}\")\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "    scraper.run()\n",
    "    logging.info(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f3b24",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd80ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 20:17:44,715 - INFO - Search button clicked.\n",
      "2024-08-14 20:17:50,645 - INFO - Search submitted.\n",
      "2024-08-14 20:17:55,879 - INFO - Category filter 'article' applied.\n",
      "2024-08-14 20:17:55,981 - INFO - Time filter applied for the last month.\n",
      "2024-08-14 20:18:01,583 - INFO - Image saved as 'news_images\\2584f646b4d713d5bf42b404d8ccc6a1.jpeg'.\n",
      "2024-08-14 20:18:02,817 - INFO - Image saved as 'news_images\\09dd5bc00ca4d679b00524a7dafe6157.jpeg'.\n",
      "2024-08-14 20:18:04,101 - INFO - Image saved as 'news_images\\fe8a23f9e06e4b532946f00f167dbb95.jpeg'.\n",
      "2024-08-14 20:18:05,531 - INFO - Image saved as 'news_images\\3edeae1dbc8c18cf4dfb6343049dea1f.jpeg'.\n",
      "2024-08-14 20:18:06,907 - INFO - Image saved as 'news_images\\26e3c79087156e34b58da64fa911883e.jpeg'.\n",
      "2024-08-14 20:18:08,242 - INFO - Image saved as 'news_images\\1f2eb21b595010b5daab36579b483d6a.jpeg'.\n",
      "2024-08-14 20:18:09,616 - INFO - Image saved as 'news_images\\c9ae12c82a6aaafe6b1ada577bd3d080.jpeg'.\n",
      "2024-08-14 20:18:10,984 - INFO - Image saved as 'news_images\\cf435fd52beb71b78b766536a2734c20.jpeg'.\n",
      "2024-08-14 20:18:12,435 - INFO - Image saved as 'news_images\\cd5806596556d70ffc1d39f4d0b2c307.jpeg'.\n",
      "2024-08-14 20:18:13,734 - INFO - Image saved as 'news_images\\74e4cf3ccf8fc85f5f319ce79cba36c8.jpeg'.\n",
      "2024-08-14 20:18:15,017 - INFO - Image saved as 'news_images\\7daa9f81caa114d22445f857375f70ed.jpeg'.\n",
      "2024-08-14 20:18:16,356 - INFO - Image saved as 'news_images\\a5104a462ae8d85b52f905a16c2eaf32.jpeg'.\n",
      "2024-08-14 20:18:17,674 - INFO - Image saved as 'news_images\\a2c84aa4f0308cda6e75d0644ce40c14.jpeg'.\n",
      "2024-08-14 20:18:19,054 - INFO - Image saved as 'news_images\\fdebf348dd61f4f96466e692a131b4c7.jpeg'.\n",
      "2024-08-14 20:18:20,398 - INFO - Image saved as 'news_images\\452b19d6512dbe537894a1d3c5d08a80.jpeg'.\n",
      "2024-08-14 20:18:21,659 - INFO - Image saved as 'news_images\\6185f817a032de9d182a20ba41d59b24.jpeg'.\n",
      "2024-08-14 20:18:22,887 - INFO - Image saved as 'news_images\\880dddc5d2affef3ffc86148481d8ce6.jpeg'.\n",
      "2024-08-14 20:18:24,196 - INFO - Image saved as 'news_images\\0d2d870417fe174a99121127b0f39659.jpeg'.\n",
      "2024-08-14 20:18:25,515 - INFO - Image saved as 'news_images\\e0085eb616f0bf3911aa40ae030c1da8.jpeg'.\n",
      "2024-08-14 20:18:27,062 - INFO - Image saved as 'news_images\\5b027c540669f50c6ce456cfef0ced87.jpeg'.\n",
      "2024-08-14 20:18:28,097 - INFO - Data saved in 'news_data.xlsx'.\n",
      "2024-08-14 20:18:30,522 - INFO - Process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import configparser\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from time import sleep\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Configuração de logs\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self, config_file='config.ini'):\n",
    "        self.config = configparser.ConfigParser()\n",
    "        self.config.read(config_file)\n",
    "        self.query = self.config.get('search', 'query')\n",
    "        self.category = self.config.get('filters', 'category')\n",
    "        self.time = self.config.get('filters', 'time')  # Mantenha como string se as comparações forem com strings\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.implicitly_wait(15)\n",
    "\n",
    "    def search_news(self):\n",
    "        url = 'https://www.news.com.au/'\n",
    "        self.driver.get(url)\n",
    "\n",
    "        try:\n",
    "            search_button = self.driver.find_element(By.XPATH, '/html/body/nav/form/button')\n",
    "            search_button.click()\n",
    "            logging.info(\"Search button clicked.\")\n",
    "\n",
    "            search_box = self.driver.find_element(By.XPATH, '/html/body/nav/form/input')\n",
    "            search_box.send_keys(self.query)\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            logging.info(\"Search submitted.\")\n",
    "            sleep(5)\n",
    "\n",
    "            filter_tab = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"refine\"]'))\n",
    "            )\n",
    "            filter_tab.click()\n",
    "\n",
    "            category_xpath_map = {\n",
    "                'article': '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[2]/label',\n",
    "                'audio': '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[3]/label',\n",
    "                'gallery': '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[4]/label',\n",
    "                'video': '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[5]/label',\n",
    "            }\n",
    "            category_xpath = category_xpath_map.get(self.category, '//*[@id=\"custom-facet-0\"]/div/div[2]/div/ul/li[1]/label')\n",
    "\n",
    "            category_filter = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, category_xpath))\n",
    "            )\n",
    "            category_filter.click()\n",
    "            logging.info(f\"Category filter '{self.category}' applied.\")\n",
    "\n",
    "            time_xpath_map = {\n",
    "                'day': '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[2]/label',\n",
    "                'week': '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[3]/label',\n",
    "                'month': '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[4]/label',\n",
    "                'year': '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[5]/label',\n",
    "            }\n",
    "            time_xpath = time_xpath_map.get(self.time, '//*[@id=\"custom-facet-1\"]/div/div[2]/div/ul/li[1]/label')\n",
    "\n",
    "            time_filter = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, time_xpath))\n",
    "            )\n",
    "            time_filter.click()\n",
    "            logging.info(f\"Time filter applied for the last {self.time}.\")\n",
    "            sleep(3)\n",
    "\n",
    "            filter_tab.click()\n",
    "            sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error applying the filters: {e}\")\n",
    "            self.driver.quit()\n",
    "            raise\n",
    "\n",
    "    def download_and_save_image(self, image_url, filename, format='JPEG'):\n",
    "        try:\n",
    "            # Baixa a imagem da URL\n",
    "            with urllib.request.urlopen(image_url) as response:\n",
    "                image_data = response.read()\n",
    "            \n",
    "            # Abre a imagem em memória\n",
    "            image = Image.open(BytesIO(image_data))\n",
    "            \n",
    "            # Salva a imagem com o nome e formato fornecidos\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)  # Certifica-se de que o diretório existe\n",
    "            image.save(filename, format=format)  # Salva a imagem com o formato especificado\n",
    "            \n",
    "            logging.info(f\"Image saved as '{filename}'.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error downloading or saving the image: {e}\")\n",
    "\n",
    "    def process_articles(self):\n",
    "        articles = self.driver.find_elements(By.CLASS_NAME, 'storyblock')\n",
    "        os.makedirs('news_images', exist_ok=True)\n",
    "        articles_data = []\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                title_element = article.find_element(By.CLASS_NAME, 'storyblock_title')\n",
    "                title = title_element.text\n",
    "\n",
    "                date_element = article.find_element(By.CLASS_NAME, 'storyblock_datetime')\n",
    "                date = date_element.get_attribute('datetime')\n",
    "\n",
    "                description_element = article.find_elements(By.CLASS_NAME, 'storyblock_standfirst')\n",
    "                description = description_element[0].text if description_element else None\n",
    "\n",
    "                image_element = article.find_element(By.CLASS_NAME, 'responsive-img_img')\n",
    "                image_url = image_element.get_attribute('src')\n",
    "\n",
    "                parsed_url = urlparse(image_url)\n",
    "                image_filename = os.path.join('news_images', os.path.basename(parsed_url.path) + \".jpeg\")\n",
    "\n",
    "                self.download_and_save_image(image_url, image_filename)\n",
    "\n",
    "                search_phrases = [self.query]\n",
    "                phrase_count = sum(title.lower().count(phrase.lower()) for phrase in search_phrases) + \\\n",
    "                               (sum(description.lower().count(phrase.lower()) for phrase in search_phrases) if description else 0)\n",
    "\n",
    "                money_pattern = re.compile(r'(\\$\\d+(\\.\\d{1,2})?)|(\\d+\\s?(dollars|USD))')\n",
    "                contains_money = bool(money_pattern.search(title) or (description and money_pattern.search(description)))\n",
    "\n",
    "                articles_data.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Date\": date,\n",
    "                    \"Description\": description,\n",
    "                    \"Picture Filename\": image_filename,\n",
    "                    \"Count of Search Phrases\": phrase_count,\n",
    "                    \"Contains Money\": contains_money\n",
    "                })\n",
    "\n",
    "                sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing the article: {e}\")\n",
    "\n",
    "        return articles_data\n",
    "\n",
    "    def save_to_excel(self, articles_data):\n",
    "        df = pd.DataFrame(articles_data)\n",
    "        df.to_excel('news_data.xlsx', index=False)\n",
    "        logging.info(\"Data saved in 'news_data.xlsx'.\")\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            self.search_news()\n",
    "            articles_data = self.process_articles()\n",
    "            self.save_to_excel(articles_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in the scraping process: {e}\")\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "    scraper.run()\n",
    "    logging.info(\"Process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
